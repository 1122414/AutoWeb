import os
import sys
import torch
import httpx
import traceback
from typing import List, Tuple, Dict, Any, Optional
from dotenv import load_dotenv

# LangChain ç›¸å…³
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.embeddings import OllamaEmbeddings
from langchain_milvus import Milvus
from langchain_core.prompts import PromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
from langchain_core.documents import Document
from langchain_core.retrievers import BaseRetriever
from langchain_core.callbacks import CallbackManagerForRetrieverRun

# --- æ··åˆæ£€ç´¢ç›¸å…³ ---
from langchain_community.retrievers import BM25Retriever

# Transformers ç›¸å…³ (ç”¨äº Rerank)
from transformers import AutoTokenizer, AutoModelForCausalLM

# é¡¹ç›®å†…éƒ¨æ¨¡å—
from config import *
from rag.query_analyzer import query_analyzer
from rag.milvus_schema import get_vector_store, FIXED_FILTERABLE_FIELDS
from rag.field_registry import get_all_filterable_fields
from prompts.rag_prompts import RAG_PROMPT

# ==============================================================================
# 0. å…¨å±€é…ç½®ä¸è®¾å¤‡æ£€æµ‹
# ==============================================================================
DEVICE = "cuda" if torch.cuda.is_available() else "cpu"
RERANK_MAX_LENGTH = 2048

# ==============================================================================
# 1. QwenReranker (é‡æ’åºæ¨¡å‹å°è£…)
# ==============================================================================


class QwenReranker:
    """
    ä½¿ç”¨ Qwen (æˆ–å…¼å®¹æ¶æ„) æ¨¡å‹è¿›è¡Œæ–‡æ¡£é‡æ’åº (Reranking)ã€‚
    å•ä¾‹æ¨¡å¼ï¼Œé¿å…é‡å¤åŠ è½½æ¨¡å‹ã€‚
    """
    _instance = None

    def __new__(cls):
        if cls._instance is None:
            print(f"ğŸš€ [System] Initializing QwenReranker on {DEVICE}...")
            cls._instance = super(QwenReranker, cls).__new__(cls)
            cls._instance.model = None
            cls._instance.tokenizer = None
            try:
                cls._instance._init_model()
            except Exception as e:
                print(f"âŒ [Reranker] Model load failed: {e}")
                print(
                    "   -> Tip: Ensure 'transformers' and 'torch' are installed and RERANK_MODEL_PATH is correct.")
        return cls._instance

    def _init_model(self):
        # å»¶è¿ŸåŠ è½½ï¼ŒèŠ‚çœèµ„æº
        self.tokenizer = AutoTokenizer.from_pretrained(
            RERANK_MODEL_PATH,
            padding_side='left',
            trust_remote_code=True
        )

        model_kwargs = {"device_map": DEVICE, "trust_remote_code": True}
        if DEVICE == "cuda":
            # æ˜¾å­˜ä¼˜åŒ–
            model_kwargs["torch_dtype"] = torch.float16

        self.model = AutoModelForCausalLM.from_pretrained(
            RERANK_MODEL_PATH,
            **model_kwargs
        ).eval()

        # é’ˆå¯¹ Qwen Instruct æ¨¡å‹çš„æ‰“åˆ† Token ID (Yes/No)
        # æ³¨æ„ï¼šä¸åŒæ¨¡å‹çš„ token id ä¸åŒï¼Œè¿™é‡Œé€‚é… Qwen/Qwen2
        self.token_false_id = self.tokenizer.convert_tokens_to_ids("no")
        self.token_true_id = self.tokenizer.convert_tokens_to_ids("yes")

        # æ„é€  Instruct Prompt (å‚è€ƒ BGE-Reranker-V2-Gemma æˆ–ç±»ä¼¼ Instruct Reranker)
        self.prefix = "<|im_start|>system\nJudge whether the Document meets the requirements based on the Query. Answer 'yes' or 'no'.<|im_end|>\n<|im_start|>user\n"
        self.suffix = "<|im_end|>\n<|im_start|>assistant\n"

        self.prefix_tokens = self.tokenizer.encode(
            self.prefix, add_special_tokens=False)
        self.suffix_tokens = self.tokenizer.encode(
            self.suffix, add_special_tokens=False)

    def _format_input(self, query: str, doc_content: str) -> str:
        return f"Query: {query}\nDocument: {doc_content[:1000]}"  # æˆªæ–­é˜²æ­¢OOM

    @torch.no_grad()
    def rerank(self, query: str, docs: List[Document], top_k: int = 5) -> List[Document]:
        if not docs or not self.model:
            return docs[:top_k]

        # æ„é€  Batch Input
        pairs = []
        for doc in docs:
            text = self._format_input(query, doc.page_content)
            pairs.append(text)

        # Tokenize
        inputs = self.tokenizer(
            pairs,
            padding=True,
            truncation=True,
            return_tensors="pt",
            max_length=RERANK_MAX_LENGTH
        ).to(self.model.device)

        # Forward pass (åªè®¡ç®— logitsï¼Œä¸ç”Ÿæˆ)
        outputs = self.model(**inputs)
        logits = outputs.logits[:, -1, :]  # å–æœ€åä¸€ä¸ª token çš„ logits

        # è®¡ç®— Yes çš„æ¦‚ç‡
        # è¿™é‡Œæ¼”ç¤ºå– yes token çš„ log_softmax
        scores = logits[:, self.token_true_id].float().cpu().numpy()

        # æ’åº
        doc_score_pairs = list(zip(docs, scores))
        doc_score_pairs.sort(key=lambda x: x[1], reverse=True)

        print(
            f"ğŸ“Š [Rerank] Top score: {doc_score_pairs[0][1]:.4f} | Low score: {doc_score_pairs[-1][1]:.4f}")

        return [doc for doc, _ in doc_score_pairs[:top_k]]

# ==============================================================================
# 2. æ ¸å¿ƒè¾…åŠ©å‡½æ•°
# ==============================================================================


# ==============================================================================
# 2. æ ¸å¿ƒè¾…åŠ©å‡½æ•°
# ==============================================================================

_cached_embedding_model = None


def get_embedding_model():
    """è‡ªåŠ¨é€‰æ‹© OpenAI æˆ– Ollama Embeddings (å•ä¾‹æ¨¡å¼)"""
    global _cached_embedding_model
    if _cached_embedding_model is not None:
        return _cached_embedding_model

    http_client = httpx.Client(trust_env=False, timeout=60.0)

    if EMBEDDING_TYPE == 'local_ollama':
        # æ¸…æ´— base_url
        base_url = OPENAI_OLLAMA_BASE_URL.replace(
            "/api/generate", "").replace("/v1", "").rstrip("/")
        instance = OllamaEmbeddings(
            base_url=base_url, model=OPENAI_OLLAMA_EMBEDDING_MODEL)

    elif EMBEDDING_TYPE == 'local_vllm':
        instance = OpenAIEmbeddings(
            model=VLLM_OPENAI_EMBEDDING_MODEL,
            openai_api_key=VLLM_OPENAI_EMBEDDING_API_KEY,
            openai_api_base=VLLM_OPENAI_EMBEDDING_BASE_URL,
            http_client=http_client,
            check_embedding_ctx_length=False
        )
    else:
        instance = OpenAIEmbeddings(
            model=EMBEDDING_MODEL,
            openai_api_key=OPENAI_API_KEY,
            openai_api_base=OPENAI_OLLAMA_BASE_URL
        )

    _cached_embedding_model = instance
    print(f"ğŸ”— [RAG] Embedding model initialized ({EMBEDDING_TYPE})")
    return _cached_embedding_model


def format_docs(docs):
    """æ ¼å¼åŒ–æ–‡æ¡£åˆ—è¡¨ä¸ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²ï¼ŒåŒ…å« metadata åŠ¨æ€å­—æ®µ"""
    parts = []
    for i, doc in enumerate(docs):
        text = f"[ç‰‡æ®µ {i+1}] {doc.page_content}"
        # é™„åŠ æœ‰æ„ä¹‰çš„ metadata
        meta_parts = []
        for k, v in doc.metadata.items():
            if v and k not in ("text", "pk", "vector") and str(v).strip():
                meta_parts.append(f"{k}: {v}")
        if meta_parts:
            text += f"\n  å…ƒæ•°æ®: {', '.join(meta_parts)}"
        parts.append(text)
    return "\n\n".join(parts)


def _cn_num_to_int(cn: str) -> int:
    """ä¸­æ–‡æ•°å­—è½¬é˜¿æ‹‰ä¼¯æ•°å­—ï¼ˆæ”¯æŒ ä¸€~ä¹åä¹ï¼‰"""
    digit_map = {"ä¸€": 1, "äºŒ": 2, "ä¸¤": 2, "ä¸‰": 3, "å››": 4, "äº”": 5,
                 "å…­": 6, "ä¸ƒ": 7, "å…«": 8, "ä¹": 9, "å": 10}
    if cn == "å":
        return 10
    result = 0
    for ch in cn:
        if ch == "å":
            result = (result or 1) * 10
        elif ch in digit_map:
            result += digit_map[ch]
    return result if result else 0


def get_retrieval_k(question: str) -> int:
    """æ ¹æ®é—®é¢˜ç±»å‹åŠ¨æ€è°ƒæ•´ Top-K"""
    import re
    # 1. è§£æ "å‰Nå/top N" â€” é˜¿æ‹‰ä¼¯æ•°å­—
    top_n_match = re.search(r'(?:å‰|top)\s*(\d+)', question, re.IGNORECASE)
    if top_n_match:
        n = int(top_n_match.group(1))
        return max(n * 2, 15)

    # 2. è§£æ "å‰åå/å‰äºŒå" â€” ä¸­æ–‡æ•°å­—
    cn_match = re.search(r'å‰([ä¸€äºŒä¸¤ä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+)', question)
    if cn_match:
        n = _cn_num_to_int(cn_match.group(1))
        if n > 0:
            return max(n * 2, 15)

    # 3. å…¨å±€æ€§æŸ¥è¯¢
    global_keywords = ["å…¨éƒ¨", "æ‰€æœ‰", "åˆ—è¡¨", "æ¸…å•",
                       "æ€»ç»“", "åˆ†æ", "all", "summary", "list"]
    if any(kw in question.lower() for kw in global_keywords):
        return 15
    return 10

# ==============================================================================
# 3. æ··åˆæ£€ç´¢æ„å»ºå™¨
# ==============================================================================


class SimpleEnsembleRetriever(BaseRetriever):
    """
    æ‰‹åŠ¨å®ç°çš„æ··åˆæ£€ç´¢å™¨ï¼Œç”¨äºæ›¿ä»£ langchain.retrievers.EnsembleRetriever
    ä½¿ç”¨åŠ æƒå€’æ•°æ’å (RRF) ç®—æ³•åˆå¹¶ç»“æœã€‚
    """
    retrievers: List[BaseRetriever]
    weights: List[float]

    def _get_relevant_documents(
        self, query: str, *, run_manager: CallbackManagerForRetrieverRun = None
    ) -> List[Document]:

        # 1. å¹¶è¡Œæˆ–ä¸²è¡Œæ‰§è¡Œæ‰€æœ‰æ£€ç´¢å™¨
        # (ç®€å•å®ç°ä¸ºä¸²è¡Œï¼Œç”Ÿäº§ç¯å¢ƒå¯ç”¨ asyncio.gather)
        doc_lists = []
        for i, retriever in enumerate(self.retrievers):
            try:
                docs = retriever.invoke(
                    query,
                    config={"callbacks": run_manager.get_child()
                            if run_manager else None}
                )
                doc_lists.append(docs)
            except Exception as e:
                print(f"âš ï¸ [SimpleEnsemble] Retriever {i} failed: {e}")
                doc_lists.append([])

        # 2. RRF (Reciprocal Rank Fusion) èåˆç®—æ³•
        # æ ¸å¿ƒæ€æƒ³ï¼šæ’åè¶Šé å‰ï¼Œåˆ†æ•°è¶Šé«˜ (1 / (rank + c))
        c = 60  # RRF å¸¸æ•°ï¼Œé€šå¸¸è®¾ä¸º 60
        scores = {}

        for docs, weight in zip(doc_lists, self.weights):
            for rank, doc in enumerate(docs):
                # ä½¿ç”¨å†…å®¹ä½œä¸º Key è¿›è¡Œå»é‡ (Milvusè¿”å›çš„IDå¯èƒ½ä¸ä¸€è‡´)
                # æ›´å¥½çš„åšæ³•æ˜¯ç”¨ hash(doc.page_content)ï¼Œè¿™é‡Œç›´æ¥ç”¨å†…å®¹å­—ç¬¦ä¸²
                key = doc.page_content

                if key not in scores:
                    scores[key] = {"doc": doc, "score": 0.0}

                # åŠ æƒåˆ†æ•°ç´¯åŠ 
                scores[key]["score"] += weight * (1 / (rank + c))

        # 3. æ ¹æ®æœ€ç»ˆ RRF åˆ†æ•°æ’åº
        sorted_results = sorted(
            scores.values(), key=lambda x: x["score"], reverse=True)

        # 4. è¿”å› Document å¯¹è±¡åˆ—è¡¨
        return [item["doc"] for item in sorted_results]


# 5. è‡ªå®šä¹‰åˆ†è¯å™¨ (ä¼˜åŒ– BM25 å¬å›)
# ==============================================================================
def custom_tokenizer(text: str) -> List[str]:
    """
    æ··åˆåˆ†è¯å™¨ (jieba + æ­£åˆ™)ï¼š
    - ä¸­æ–‡éƒ¨åˆ†ï¼šjieba ç²¾ç¡®æ¨¡å¼åˆ†è¯ (äººå·¥æ™ºèƒ½ â†’ [äººå·¥, æ™ºèƒ½])
    - è‹±æ–‡/æ•°å­—éƒ¨åˆ†ï¼šæ­£åˆ™æŒ‰éå­—æ¯æ•°å­—ç¬¦å·åˆ‡åˆ† (kimi-2.5 â†’ [kimi, 2, 5])
    - æ‰€æœ‰ token è½¬å°å†™ã€å»ç©º
    """
    import re
    import jieba

    text = text.lower()
    tokens = []

    # æŒ‰ä¸­æ–‡ vs éä¸­æ–‡äº¤æ›¿åˆ‡åˆ†
    segments = re.findall(r'[\u4e00-\u9fa5]+|[^\u4e00-\u9fa5]+', text)

    for seg in segments:
        if re.match(r'[\u4e00-\u9fa5]', seg):
            # ä¸­æ–‡æ®µ â†’ jieba åˆ†è¯
            tokens.extend(jieba.lcut(seg))
        else:
            # è‹±æ–‡/æ•°å­—æ®µ â†’ æ­£åˆ™æŒ‰ç¬¦å·åˆ‡åˆ†
            parts = re.split(r'[^a-zA-Z0-9]+', seg)
            tokens.extend(parts)

    return [t for t in tokens if t.strip()]


def build_hybrid_retriever(milvus_store: Milvus, k: int):
    """
    æ„å»ºæ··åˆæ£€ç´¢å™¨ï¼šMilvus (Dense) + BM25 (Sparse)
    """
    # 1. å‡†å¤‡ Milvus æ£€ç´¢å™¨ (Dense - è¯­ä¹‰æ£€ç´¢)
    milvus_retriever = milvus_store.as_retriever(
        search_type="mmr",  # ä½¿ç”¨ MMR å¢åŠ å¤šæ ·æ€§
        # åç»­å¯ä»¥é€šè¿‡categoryç­‰ç­›é€‰åšæ··åˆæ£€ç´¢
        search_kwargs={
            "k": k,
            "fetch_k": k * 2,
            "lambda_mult": 0.6
        }
    )

    # 2. å‡†å¤‡ BM25 æ£€ç´¢å™¨ (Sparse - å…³é”®è¯æ£€ç´¢)
    print("â³ [Hybrid] æ„å»ºä¸´æ—¶ BM25 ç´¢å¼• (In-Memory)...")
    bm25_retriever = None

    try:
        # 3. ä¼˜åŒ–é‡‡æ ·ç­–ç•¥ï¼šä¼˜å…ˆæ‹‰å–æœ€æ–°çš„æ•°æ® (pk desc)
        # BM25 æ„å»ºä½¿ç”¨å…¨é‡æ•°æ® (pk >= 0)ï¼Œå› ä¸º filter_expr å¯èƒ½ä¸å‡†ç¡®
        output_fields = ["text"] + list(FIXED_FILTERABLE_FIELDS)

        try:
            print(f" ğŸ›¡ï¸ [BM25] Query with pk >= 0")
            # å¢åŠ  limit åˆ° 5000 ä»¥è¦†ç›–æ›´å¤šæ•°æ® (è§†å†…å­˜æƒ…å†µè°ƒæ•´)
            res = milvus_store.col.query(
                expr="pk >= 0",
                output_fields=output_fields,
                limit=3000,
                offset=0
            )
            print(f"   âœ… [BM25] query returned {len(res)} docs")
        except Exception as e:
            print(f"   âš ï¸ [BM25] query failed: {e}")
            res = []

        if res:
            bm25_docs = []
            for r in res:
                # é‡å»º Document å¯¹è±¡ï¼ˆåŠ¨æ€æå–å›ºå®šå­—æ®µï¼‰
                meta = {f: r.get(f, "") for f in FIXED_FILTERABLE_FIELDS}
                # Milvus LangChain é»˜è®¤æŠŠ content å­˜åœ¨ 'text' å­—æ®µ
                text_content = r.get("text") or r.get("page_content") or ""
                if text_content:
                    bm25_docs.append(
                        Document(page_content=text_content, metadata=meta))

            if bm25_docs:
                # æ³¨å…¥è‡ªå®šä¹‰åˆ†è¯å™¨
                bm25_retriever = BM25Retriever.from_documents(
                    bm25_docs,
                    preprocess_func=custom_tokenizer
                )
                bm25_retriever.k = k  # è®¾ç½® BM25 çš„å¬å›æ•°é‡
                print(
                    f"   -> BM25 ç´¢å¼•æ„å»ºå®Œæˆ (Docs: {len(bm25_docs)}) | Tokenizer: Regex")
            else:
                print("   -> Milvus è¿”å›æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡ BM25")
        else:
            print("   -> æ— æ³•ä» Milvus æ‹‰å–æ•°æ®ï¼Œè·³è¿‡ BM25")

    except Exception as e:
        print(f"âš ï¸ [Hybrid] BM25 æ„å»ºå¤±è´¥ (é™çº§ä¸ºçº¯å‘é‡æ£€ç´¢): {e}")

    # 3. ç»„åˆ (Custom Ensemble)
    if bm25_retriever:
        print("ğŸ”— [Hybrid] å¯ç”¨æ··åˆæ£€ç´¢: Milvus(0.5) + BM25(0.5)")
        # ä½¿ç”¨æˆ‘ä»¬è‡ªå®šä¹‰çš„ SimpleEnsembleRetriever
        return SimpleEnsembleRetriever(
            retrievers=[milvus_retriever, bm25_retriever],
            weights=[0.5, 0.5]  # æƒé‡å¯è°ƒï¼Œ0.5/0.5 æ˜¯æ¯”è¾ƒå‡è¡¡çš„èµ·ç‚¹
        )
    else:
        print("âš ï¸ [Hybrid] ä»…ä½¿ç”¨ Milvus å‘é‡æ£€ç´¢")
        return milvus_retriever

# ==============================================================================
# 4. RAG ä¸»æµç¨‹
# ==============================================================================


def _generate_answer(question: str, docs: List[Document]) -> str:
    """é€šç”¨ç”Ÿæˆå‡½æ•°"""
    llm = ChatOpenAI(
        model=MODEL_NAME,
        temperature=0.1,
        max_tokens=4096,  # é˜²æ­¢é•¿è¡¨æ ¼å›ç­”è¢«æˆªæ–­
        openai_api_key=OPENAI_API_KEY,
        openai_api_base=OPENAI_BASE_URL
    )

    if RAG_PROMPT:
        if isinstance(RAG_PROMPT, str):
            custom_rag_prompt = PromptTemplate.from_template(RAG_PROMPT)
        else:
            custom_rag_prompt = RAG_PROMPT
    else:
        # é»˜è®¤ Prompt
        template = """åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡å›ç­”é—®é¢˜ã€‚å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆï¼Œè¯·ç›´æ¥è¯´ä¸çŸ¥é“ã€‚\n\nä¸Šä¸‹æ–‡ï¼š\n{context}\n\né—®é¢˜ï¼š{question}"""
        custom_rag_prompt = PromptTemplate.from_template(template)

    formatted_context = format_docs(docs)

    print("ğŸ“ [Generate] Generating answer...")
    chain = (
        custom_rag_prompt
        | llm
        | StrOutputParser()
    )

    return chain.invoke({"context": formatted_context, "question": question})


def _handle_sort_query(question: str, analysis: Dict) -> str:
    """å¤„ç†æ’åºç±»æŸ¥è¯¢ (ç›´æ¥æŸ¥åº“ + æ’åº)"""
    sort_field = analysis['sort_field']
    sort_order = analysis['sort_order']
    print(f"ğŸ“‰ [Sort Path] Field: {sort_field} | Order: {sort_order}")

    embeddings = get_embedding_model()
    vector_store = get_vector_store(embeddings)

    try:
        # 1. æ‹‰å–æ•°æ® (Limit 500 for memory safety)
        # è·å–æ‰€æœ‰ç›¸å…³å­—æ®µä»¥ä¾›å±•ç¤º
        output_fields = ["text"] + list(FIXED_FILTERABLE_FIELDS)
        if sort_field not in output_fields:
            output_fields.append(sort_field)

        print(f"   ğŸ” Querying Milvus for sort: pk >= 0 (limit=500)")
        res = vector_store.col.query(
            expr="pk >= 0",
            output_fields=output_fields,
            limit=500  # é™åˆ¶æ’åºæ•°æ®é‡
        )

        if not res:
            return "âŒ çŸ¥è¯†åº“ä¸ºç©ºï¼Œæ— æ³•è¿›è¡Œæ’åºã€‚"

        # 2. Python å†…å­˜æ’åº
        def get_sort_val(item):
            val = item.get(sort_field)
            if val is None:
                return -float('inf') if sort_order == 'desc' else float('inf')
            try:
                return float(val)
            except ValueError:
                return str(val)

        reverse = (sort_order.lower() == "desc")
        sorted_res = sorted(res, key=get_sort_val, reverse=reverse)

        # 3. æˆªå– Top-K å¹¶è½¬æ¢ä¸º Documents
        k = get_retrieval_k(question)
        top_res = sorted_res[:k]

        docs = []
        for r in top_res:
            meta = {f: r.get(f, "") for f in FIXED_FILTERABLE_FIELDS}
            meta[sort_field] = r.get(sort_field, "")  # ç¡®ä¿æ’åºå­—æ®µå¯è§

            text = r.get("text") or r.get("page_content") or ""
            if text:
                docs.append(Document(page_content=text, metadata=meta))

        if not docs:
            return "âŒ æœªæ‰¾åˆ°æœ‰æ•ˆæ•°æ®è¿›è¡Œæ’åºã€‚"

        # 4. ç”Ÿæˆå›ç­”
        return _generate_answer(question, docs)

    except Exception as e:
        traceback.print_exc()
        return f"æ’åºæŸ¥è¯¢å¤„ç†å¤±è´¥: {str(e)}"


def _handle_semantic_query(question: str, analysis: Dict) -> str:
    """å¤„ç†è¯­ä¹‰æ£€ç´¢æŸ¥è¯¢ (RAG æµç¨‹)"""
    search_query = analysis['search_query']
    print(f"ğŸ§  [Semantic Path] Query: {search_query}")

    embeddings = get_embedding_model()
    vector_store = get_vector_store(embeddings)

    # 1. Recall (Hybrid)
    target_k = get_retrieval_k(question)
    recall_k = target_k * 3

    # æ³¨æ„ï¼šä¸å†ä¼ å…¥ filter_expr
    hybrid_retriever = build_hybrid_retriever(vector_store, recall_k)

    print(f"ğŸ” [Retrieve] Fetching candidates...")
    initial_docs = hybrid_retriever.invoke(search_query)

    if not initial_docs:
        return "âŒ æ²¡æœ‰åœ¨çŸ¥è¯†åº“ä¸­æ‰¾åˆ°ç›¸å…³ä¿¡æ¯ã€‚"

    # 2. Deduplicate
    unique_docs = []
    seen_content = set()
    for doc in initial_docs:
        fingerprint = doc.page_content[:100]
        if fingerprint not in seen_content:
            unique_docs.append(doc)
            seen_content.add(fingerprint)

    print(f"   -> Retrieved {len(unique_docs)} unique docs.")

    # 3. Rerank
    print(f"âš–ï¸ [Rerank] ä½¿ç”¨ QwenReranker è¿›è¡Œç²¾æ’...")
    try:
        reranker = QwenReranker()
        final_docs = reranker.rerank(question, unique_docs, top_k=target_k)
    except Exception as e:
        print(f"âš ï¸ Rerank failed: {e}, using raw retrieval results.")
        final_docs = unique_docs[:target_k]

    # 4. Generate
    return _generate_answer(question, final_docs)


def qa_interaction(question: str) -> str:
    print(f"\nğŸ¤” [RAG] Searching for: {question}")

    # A. æ„å›¾åˆ†æ
    analysis = {
        "filter_expr": "",
        "search_query": question,
        "sort_field": "",
        "sort_order": ""
    }

    if query_analyzer:
        try:
            # ä½¿ç”¨æ–°çš„ analyze æ–¹æ³•
            analysis = query_analyzer.analyze(question)
        except Exception as e:
            print(f"âš ï¸ Query analysis failed: {e}")

    # B. åˆ†å‘é€»è¾‘
    if analysis.get("sort_field"):
        return _handle_sort_query(question, analysis)
    else:
        return _handle_semantic_query(question, analysis)


if __name__ == "__main__":
    # æµ‹è¯•å…¥å£
    q = sys.argv[1] if len(sys.argv) > 1 else "æµ‹è¯•ï¼šä»‹ç»ä¸€ä¸‹ç³»ç»Ÿé‡Œçš„ç”µå½±"
    print(qa_interaction(q))
