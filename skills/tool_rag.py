"""
AutoWeb çŸ¥è¯†åº“ç®¡ç†å™¨
====================
åŠŸèƒ½ï¼š
- å•ä¾‹æ¨¡å¼ç®¡ç† Milvus è¿æ¥å’Œ Embedding æ¨¡å‹
- ç¼“å†²é˜Ÿåˆ— + æ‰¹é‡å¼‚æ­¥å†™å…¥
- ç¨‹åºé€€å‡ºæ—¶åŒæ­¥åˆ·æ–°
"""
import sys
import os
import atexit
from typing import List, Dict, Union, Optional
from concurrent.futures import ThreadPoolExecutor, Future
from threading import Lock


# ç¡®ä¿é¡¹ç›®æ ¹ç›®å½•åœ¨ path ä¸­
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))


class KnowledgeBaseManager:
    """
    çŸ¥è¯†åº“ç®¡ç†å™¨ï¼ˆå•ä¾‹ï¼‰

    ä½¿ç”¨æ–¹å¼:
        from skills.tool_rag import kb_manager
        kb_manager.add("çˆ¬å–çš„æ–‡æœ¬å†…å®¹", source="https://example.com")
        kb_manager.flush_and_wait()  # ç¨‹åºé€€å‡ºå‰è°ƒç”¨
    """
    _instance: Optional['KnowledgeBaseManager'] = None
    _initialized: bool = False

    # é…ç½®
    BUFFER_THRESHOLD = 10  # ç¼“å†²åŒºé˜ˆå€¼ï¼Œè¾¾åˆ°åè‡ªåŠ¨åˆ·æ–°
    MAX_CONTENT_LENGTH = 5000  # å•æ¡å†…å®¹æœ€å¤§é•¿åº¦

    def __new__(cls):
        if cls._instance is None:
            cls._instance = super().__new__(cls)
        return cls._instance

    def __init__(self):
        if KnowledgeBaseManager._initialized:
            return
        KnowledgeBaseManager._initialized = True

        self.buffer: List = []  # å¾…å†™å…¥çš„æ–‡æ¡£ç¼“å†²
        self.lock = Lock()  # çº¿ç¨‹å®‰å…¨é”
        self.executor = ThreadPoolExecutor(
            max_workers=1, thread_name_prefix="kb_writer")
        self.pending_futures: List[Future] = []  # è·Ÿè¸ªå¼‚æ­¥ä»»åŠ¡

        # å»¶è¿Ÿåˆå§‹åŒ–ï¼ˆé¦–æ¬¡ä½¿ç”¨æ—¶æ‰è¿æ¥ï¼‰
        self._embeddings = None
        self._vector_store = None

        # æ³¨å†Œç¨‹åºé€€å‡ºæ—¶çš„æ¸…ç†å‡½æ•°
        atexit.register(self._cleanup)

        print("ğŸ“š [KnowledgeBaseManager] åˆå§‹åŒ–å®Œæˆï¼ˆå»¶è¿ŸåŠ è½½æ¨¡å¼ï¼‰")

    def _ensure_connection(self):
        """ç¡®ä¿è¿æ¥å·²å»ºç«‹ï¼ˆå»¶è¿Ÿåˆå§‹åŒ–ï¼‰"""
        if self._embeddings is None:
            print("ğŸ”Œ [KnowledgeBaseManager] å»ºç«‹ Embedding å’Œ Milvus è¿æ¥...")
            try:
                from config import MILVUS_URI
                from rag.retriever_qa import get_embedding_model
                from rag.milvus_schema import get_vector_store
                from skills.vector_gateway import connect_milvus

                connect_milvus(MILVUS_URI, alias="default",
                               tag="KnowledgeBaseManager")
                self._embeddings = get_embedding_model()
                self._vector_store = get_vector_store(self._embeddings)
                print("   âœ… è¿æ¥å»ºç«‹æˆåŠŸï¼ˆSchema å·²éªŒè¯ï¼‰")
            except Exception as e:
                print(f"   âŒ è¿æ¥å¤±è´¥: {e}")
                raise

    # é«˜é¢‘å­—æ®µååˆ—è¡¨ï¼ˆä¸ milvus_schema.py ä¸­çš„å›ºå®šå­—æ®µä¿æŒä¸€è‡´ï¼‰
    HIGH_FREQ_FIELDS = ["source", "title", "category",
                        "data_type", "platform", "crawled_at"]

    @staticmethod
    def _convert_dynamic_value(value):
        """
        å¯¹åŠ¨æ€å­—æ®µå€¼åšæ™ºèƒ½ç±»å‹è½¬æ¢

        Returns:
            (converted_value, was_percent: bool)
            - "41.30"  â†’ (41.30,  False)
            - "80.0%"  â†’ (80.0,   True)   â† æ ‡è®°ä¸ºç™¾åˆ†å·æ¥æº
            - "Â¥4.32"  â†’ (4.32,   False)
            - "-"      â†’ (None,   False)
            - çº¯æ–‡æœ¬   â†’ ("text", False)
        """
        if not isinstance(value, str):
            return value, False  # int / float / bool ç›´æ¥è¿”å›

        stripped = value.strip()

        # æ— æ•ˆå€¼ â†’ ä¸å­˜å…¥
        if stripped in ("", "-", "--", "N/A", "n/a", "null", "None"):
            return None, False

        # å»æ‰è´§å¸ç¬¦å·
        cleaned = stripped
        for prefix in ("Â¥", "$", "â‚¬", "Â£", "ï¿¥"):
            if cleaned.startswith(prefix):
                cleaned = cleaned[len(prefix):].strip()
                break

        # æ£€æµ‹å¹¶å»æ‰ç™¾åˆ†å·
        was_percent = False
        if cleaned.endswith("%"):
            cleaned = cleaned[:-1].strip()
            was_percent = True

        # å»æ‰åƒåˆ†ä½é€—å·: "1,234.56" â†’ "1234.56"
        cleaned = cleaned.replace(",", "")

        # å°è¯•è½¬ä¸ºæ•°å­—
        try:
            return float(cleaned), was_percent
        except (ValueError, TypeError):
            pass

        return stripped, False  # çº¯æ–‡æœ¬ä¿æŒå­—ç¬¦ä¸²

    def _extract_metadata(self, item: Dict, source: str) -> Dict:
        """
        ä»å­—å…¸æ•°æ®ä¸­æå– metadata

        é«˜é¢‘å­—æ®µæ”¾å…¥å¯¹åº” keyï¼Œå…¶ä»–å­—æ®µä¹Ÿæ”¾å…¥ metadataï¼ˆåŠ¨æ€å­—æ®µï¼‰ï¼Œ
        è‡ªåŠ¨æ³¨å…¥ crawled_at æ—¶é—´æˆ³ã€‚
        åŠ¨æ€å­—æ®µå€¼åšæ™ºèƒ½ç±»å‹è½¬æ¢ï¼ˆå­—ç¬¦ä¸²æ•°å­—â†’floatï¼‰ï¼Œæ— æ•ˆå€¼ä¸å­˜å…¥ã€‚
        """
        from datetime import datetime
        metadata = {}

        # æ³¨å…¥é«˜é¢‘å­—æ®µï¼ˆæœ‰åˆ™å–å€¼ï¼Œæ— åˆ™ç•™ç©ºè®© Schema é»˜è®¤å€¼å¤„ç†ï¼‰
        metadata["source"] = item.get("source", source)
        metadata["title"] = item.get("title", item.get("name", ""))
        metadata["category"] = item.get("category", item.get("type", ""))
        metadata["data_type"] = item.get("data_type", "crawled")
        metadata["platform"] = item.get("platform", "")
        metadata["crawled_at"] = item.get(
            "crawled_at", datetime.now().strftime("%Y-%m-%d %H:%M:%S"))

        # å…¶ä»–å­—æ®µä¹Ÿæ”¾å…¥ metadataï¼ˆåˆ©ç”¨ Milvus åŠ¨æ€å­—æ®µï¼‰
        pct_fields = []  # è®°å½•åŸå§‹å€¼å¸¦ % çš„å­—æ®µå
        for key, value in item.items():
            if key not in self.HIGH_FREQ_FIELDS and key not in ("text", "content", "page_content"):
                # åªå­˜æ ‡é‡å€¼ï¼Œè·³è¿‡åµŒå¥—ç»“æ„
                if isinstance(value, (str, int, float, bool)):
                    converted, was_pct = self._convert_dynamic_value(value)
                    if converted is not None:  # è·³è¿‡æ— æ•ˆå€¼
                        metadata[key] = converted
                        if was_pct:
                            pct_fields.append(key)

        # å­˜å‚¨ç™¾åˆ†å·æ ‡è®°ï¼ˆæ‰¹æ¬¡æ£€æµ‹æ—¶ç”¨ï¼Œå†™å…¥ Milvus å‰ä¼šæ¸…ç†ï¼‰
        if pct_fields:
            metadata["_pct_fields"] = pct_fields

        return metadata

    @staticmethod
    def _sanitize_format_consistency(docs: list, min_samples: int = 3):
        """
        æ‰¹æ¬¡å†…æ ¼å¼ä¸€è‡´æ€§æ£€æŸ¥

        é’ˆå¯¹åœºæ™¯ï¼šåŒä¸€å­—æ®µä¸­å¤§éƒ¨åˆ†å€¼æ˜¯çº¯ floatï¼Œå°‘æ•°å€¼åŸå§‹å¸¦ %
        è§„åˆ™ï¼šå¦‚æœå¸¦ % çš„å€¼åœ¨è¯¥å­—æ®µä¸­æ˜¯å°‘æ•°æ´¾ (< 50%)ï¼Œåˆ™è§†ä¸ºæ ¼å¼å¼‚å¸¸å¹¶ç§»é™¤
        """
        # 1. ç»Ÿè®¡æ¯ä¸ªæ•°å€¼å­—æ®µçš„ pct / non-pct åˆ†å¸ƒ
        # {field: {"total": N, "pct_count": M, "pct_docs": [(doc_idx, val), ...]}}
        field_stats = {}
        for i, doc in enumerate(docs):
            pct_fields = set(doc.metadata.get("_pct_fields", []))
            for k, v in doc.metadata.items():
                if k.startswith("_") or not isinstance(v, (int, float)):
                    continue
                if k not in field_stats:
                    field_stats[k] = {"total": 0,
                                      "pct_count": 0, "pct_docs": []}
                field_stats[k]["total"] += 1
                if k in pct_fields:
                    field_stats[k]["pct_count"] += 1
                    field_stats[k]["pct_docs"].append((i, v))

        # 2. å¯¹ % å°‘æ•°æ´¾å­—æ®µï¼Œç§»é™¤å…¶å¼‚å¸¸å€¼
        removed = 0
        for field, stats in field_stats.items():
            if stats["total"] < min_samples or stats["pct_count"] == 0:
                continue
            # å¸¦ % çš„æ˜¯å°‘æ•°æ´¾ â†’ æ ¼å¼ä¸ä¸€è‡´ â†’ ç§»é™¤
            if stats["pct_count"] / stats["total"] < 0.5:
                for doc_idx, val in stats["pct_docs"]:
                    if field in docs[doc_idx].metadata:
                        del docs[doc_idx].metadata[field]
                        removed += 1
                        print(
                            f"âš ï¸ [KB] æ ¼å¼ä¸ä¸€è‡´: {field}={val}"
                            f" (åŸå§‹å¸¦ %, ä¸åŒå­—æ®µå…¶ä»–çº¯æ•°å€¼ä¸ä¸€è‡´)ï¼Œå·²ç§»é™¤"
                        )

        # 3. æ¸…ç†å†…éƒ¨æ ‡è®° _pct_fieldsï¼ˆä¸å†™å…¥ Milvusï¼‰
        for doc in docs:
            doc.metadata.pop("_pct_fields", None)

        if removed:
            print(f"ğŸ§¹ [KB] æœ¬æ‰¹æ¬¡å…±æ¸…ç† {removed} ä¸ªæ ¼å¼å¼‚å¸¸å€¼")

    def _get_text_content(self, item) -> str:
        """
        ä»æ•°æ®ä¸­æå– page_content æ–‡æœ¬

        ä¼˜å…ˆçº§ï¼štext > content > page_content > JSON åºåˆ—åŒ–
        """
        import json
        if isinstance(item, str):
            return item
        if isinstance(item, dict):
            # ä¼˜å…ˆå–ä¸“ç”¨æ–‡æœ¬å­—æ®µ
            for key in ("text", "content", "page_content", "description", "summary"):
                if key in item and item[key]:
                    return str(item[key])
            # æ²¡æœ‰ä¸“ç”¨å­—æ®µï¼Œåºåˆ—åŒ–æ•´ä¸ª dict
            return json.dumps(item, ensure_ascii=False, indent=2)
        return str(item)

    def add(self, content: Union[str, Dict, List], source: str = "auto_crawl") -> bool:
        """
        æ·»åŠ å†…å®¹åˆ°ç¼“å†²åŒºï¼ˆéé˜»å¡ï¼‰

        Args:
            content: æ–‡æœ¬å†…å®¹ã€å­—å…¸æˆ–å­—å…¸åˆ—è¡¨
            source: æ•°æ®æ¥æºæ ‡è¯†

        Returns:
            bool: æ˜¯å¦æˆåŠŸåŠ å…¥ç¼“å†²
        """
        from langchain_core.documents import Document
        from rag.field_registry import register_fields
        from datetime import datetime

        try:
            # ç»Ÿä¸€è½¬æ¢ä¸ºåˆ—è¡¨
            items = []
            if isinstance(content, str):
                items = [content]
            elif isinstance(content, dict):
                items = [content]
            elif isinstance(content, list):
                items = content

            docs = []
            all_field_samples = {}  # {field_name: sample_value} ç”¨äºæ¨æ–­ç±»å‹

            for item in items:
                # æå–æ–‡æœ¬ï¼ˆå†…å®¹ï¼‰
                text = self._get_text_content(item)
                if len(text) < 10:
                    continue
                if len(text) > self.MAX_CONTENT_LENGTH:
                    text = text[:self.MAX_CONTENT_LENGTH] + "...[æˆªæ–­]"

                # æ„å»º metadata
                if isinstance(item, dict):
                    metadata = self._extract_metadata(item, source)
                    # æ”¶é›†å­—æ®µæ ·æœ¬å€¼ï¼ˆç”¨äºæ¨æ–­ç±»å‹ï¼‰
                    for k, v in metadata.items():
                        if k not in all_field_samples:
                            all_field_samples[k] = v
                else:
                    metadata = {
                        "source": source,
                        "title": "",
                        "category": "",
                        "data_type": "crawled",
                        "platform": "",
                        "crawled_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                    }

                docs.append(Document(page_content=text, metadata=metadata))

            if not docs:
                return False

            # æ‰¹æ¬¡å†…æ ¼å¼ä¸€è‡´æ€§æ£€æŸ¥ (% vs çº¯æ•°å€¼)
            self._sanitize_format_consistency(docs)

            # æ³¨å†Œå­—æ®µåˆ°æ³¨å†Œè¡¨ï¼ˆå«ç±»å‹ä¿¡æ¯ï¼‰
            if all_field_samples:
                register_fields(all_field_samples)

            with self.lock:
                self.buffer.extend(docs)
                buffer_size = len(self.buffer)

            print(
                f"ğŸ“¥ [KB] å·²åŠ å…¥ç¼“å†² ({buffer_size} æ¡å¾…å†™å…¥, å­—æ®µ: {len(all_field_samples)} ä¸ª)")

            # è¾¾åˆ°é˜ˆå€¼è‡ªåŠ¨åˆ·æ–°
            if buffer_size >= self.BUFFER_THRESHOLD:
                self.flush_async()

            return True

        except Exception as e:
            print(f"âŒ [KB] æ·»åŠ å¤±è´¥: {e}")
            return False

    def flush_async(self) -> Optional[Future]:
        """
        å¼‚æ­¥åˆ·æ–°ç¼“å†²åŒºï¼ˆéé˜»å¡ï¼‰

        Returns:
            Future: å¼‚æ­¥ä»»åŠ¡å¥æŸ„ï¼Œå¯ç”¨äºç­‰å¾…å®Œæˆ
        """
        with self.lock:
            if not self.buffer:
                return None
            docs_to_save = self.buffer.copy()
            self.buffer.clear()

        print(f"ğŸš€ [KB] å¼‚æ­¥å†™å…¥ {len(docs_to_save)} æ¡æ•°æ®...")
        future = self.executor.submit(self._save_batch, docs_to_save)
        self.pending_futures.append(future)

        # æ¸…ç†å·²å®Œæˆçš„ Future
        self.pending_futures = [
            f for f in self.pending_futures if not f.done()]

        return future

    def _save_batch(self, docs: List) -> bool:
        """æ‰¹é‡å†™å…¥ï¼ˆåœ¨çº¿ç¨‹æ± ä¸­æ‰§è¡Œï¼‰"""
        try:
            from skills.vector_gateway import add_documents
            self._ensure_connection()
            add_documents(self._vector_store, docs, tag="KnowledgeBaseManager")
            # æ˜¾å¼ flushï¼Œç¡®ä¿æ•°æ®æŒä¹…åŒ–åˆ° Milvus
            if self._vector_store and hasattr(self._vector_store, 'col') and self._vector_store.col:
                self._vector_store.col.flush()
                print(f"   âœ… [KB] æˆåŠŸå†™å…¥å¹¶ flush {len(docs)} æ¡æ•°æ®")
            else:
                print(f"   âœ… [KB] æˆåŠŸå†™å…¥ {len(docs)} æ¡æ•°æ®ï¼ˆæ— æ³• flushï¼‰")
            return True
        except Exception as e:
            print(f"   âŒ [KB] æ‰¹é‡å†™å…¥å¤±è´¥: {e}")
            return False

    def flush_and_wait(self, timeout: float = 120.0) -> bool:
        """
        åŒæ­¥åˆ·æ–°å¹¶ç­‰å¾…æ‰€æœ‰å¼‚æ­¥ä»»åŠ¡å®Œæˆï¼ˆç¨‹åºé€€å‡ºæ—¶è°ƒç”¨ï¼‰

        Args:
            timeout: æœ€å¤§ç­‰å¾…æ—¶é—´ï¼ˆç§’ï¼‰

        Returns:
            bool: æ˜¯å¦å…¨éƒ¨å®Œæˆ
        """
        print("â³ [KB] æ­£åœ¨åˆ·æ–°ç¼“å†²åŒºå¹¶ç­‰å¾…æ‰€æœ‰å†™å…¥å®Œæˆ...")

        # å…ˆåˆ·æ–°å½“å‰ç¼“å†²
        self.flush_async()

        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        from concurrent.futures import wait, FIRST_EXCEPTION

        if self.pending_futures:
            done, not_done = wait(self.pending_futures, timeout=timeout)

            if not_done:
                print(f"   âš ï¸ [KB] {len(not_done)} ä¸ªä»»åŠ¡è¶…æ—¶æœªå®Œæˆ")
                return False

            # æ£€æŸ¥æ˜¯å¦æœ‰å¼‚å¸¸
            for future in done:
                try:
                    future.result()
                except Exception as e:
                    print(f"   âŒ [KB] ä»»åŠ¡å¼‚å¸¸: {e}")

        print("   âœ… [KB] æ‰€æœ‰å†™å…¥ä»»åŠ¡å·²å®Œæˆ")
        return True

    def _cleanup(self):
        """ç¨‹åºé€€å‡ºæ—¶çš„æ¸…ç†ï¼ˆatexit å›è°ƒï¼‰"""
        print("\nğŸ”„ [KB] ç¨‹åºé€€å‡ºï¼Œæ­£åœ¨æ¸…ç†...")
        self.flush_and_wait(timeout=10.0)
        self.executor.shutdown(wait=False)


# ==================== å…¨å±€å•ä¾‹ ====================
kb_manager = KnowledgeBaseManager()


# ==================== ä¾¿æ·å‡½æ•°ï¼ˆå‘åå…¼å®¹ï¼‰====================

def ask_knowledge_base(question: str) -> str:
    """
    [RAG] æŸ¥è¯¢æœ¬åœ°çŸ¥è¯†åº“ã€‚

    Args:
        question(str): ç”¨æˆ·çš„è‡ªç„¶è¯­è¨€é—®é¢˜ï¼ˆå®Œæ•´é—®é¢˜ï¼Œå†…éƒ¨å¤„ç†åˆ†æï¼‰ã€‚

    Returns:
        str: çŸ¥è¯†åº“çš„å›ç­”ã€‚
    """
    print(f"ğŸ“š [RAG] æ­£åœ¨æŸ¥è¯¢çŸ¥è¯†åº“: {question}")

    try:
        from rag.retriever_qa import qa_interaction
        answer = qa_interaction(question)
        return answer
    except ImportError as e:
        return f"Error: RAG æ¨¡å—æœªæ‰¾åˆ°æˆ–å¯¼å…¥å¤±è´¥ã€‚{e}"
    except Exception as e:
        return f"Error: æŸ¥è¯¢çŸ¥è¯†åº“æ—¶å‡ºé”™: {e}"


def save_to_knowledge_base(content: str, source: str = "auto_web_spider") -> bool:
    """
    [RAG] å°†å†…å®¹ä¿å­˜åˆ°çŸ¥è¯†åº“ï¼ˆå¼‚æ­¥éé˜»å¡ï¼‰

    Args:
        content: æ–‡æœ¬å†…å®¹
        source: æ•°æ®æ¥æº

    Returns:
        bool: æ˜¯å¦æˆåŠŸåŠ å…¥ç¼“å†²
    """
    return kb_manager.add(content, source)
