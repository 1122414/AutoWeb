# ==============================================================================
# VectorCacheBase - å‘é‡ç¼“å­˜ç®¡ç†å™¨åŸºç±»
# ==============================================================================
# æå–è‡ª CodeCacheManager å’Œ DomCacheManager çš„å…¬å…±é€»è¾‘:
#   è¿æ¥ç®¡ç†, Schema æ ¡éªŒ, Embedding è·å–, URL æ ‡å‡†åŒ–,
#   DOM å“ˆå¸Œ, ç¼“å­˜å¤±æ•ˆ, çº¿ç¨‹æ± ç®¡ç†
# ==============================================================================
import atexit
import hashlib
import re
from abc import ABC, abstractmethod
from concurrent.futures import ThreadPoolExecutor
from typing import Dict, List, Optional, Tuple

from pymilvus import (
    AnnSearchRequest,
    Collection,
    CollectionSchema,
    DataType,
    FieldSchema,
    WeightedRanker,
    utility,
)
from urllib.parse import urlparse

from config import MILVUS_URI
from skills.vector_gateway import (
    connect_milvus,
    hybrid_search,
    insert_and_flush,
    normalize_weights,
    read_hit_field,
)


class VectorCacheBase(ABC):
    """å‘é‡ç¼“å­˜ç®¡ç†å™¨çš„æŠ½è±¡åŸºç±»ï¼Œå°è£…ä¸ Milvus äº¤äº’çš„é€šç”¨é€»è¾‘ã€‚"""

    def __init__(self, weights: Tuple, defaults: Tuple, tag: str):
        self._collection: Optional[Collection] = None
        self._embeddings = None
        self._vector_dim: Optional[int] = None
        self._tag = tag
        self._weights = normalize_weights(weights, defaults=defaults, tag=tag)
        self._executor = ThreadPoolExecutor(
            max_workers=1, thread_name_prefix=tag
        )
        atexit.register(self._shutdown)

    # ------------------------------------------------------------------
    # æŠ½è±¡æ–¹æ³• â€”â€” å­ç±»å¿…é¡»å®ç°
    # ------------------------------------------------------------------

    @property
    @abstractmethod
    def _collection_name(self) -> str:
        """Milvus Collection åç§°"""

    @property
    @abstractmethod
    def _collection_description(self) -> str:
        """Milvus Collection æè¿°"""

    @abstractmethod
    def _schema_fields(self, dim: int) -> List[FieldSchema]:
        """è¿”å› Schema å­—æ®µåˆ—è¡¨"""

    @abstractmethod
    def _vector_field_names(self) -> List[str]:
        """è¿”å›æ‰€æœ‰å‘é‡å­—æ®µååˆ—è¡¨ï¼Œç”¨äº Schema æ ¡éªŒå’Œç´¢å¼•åˆ›å»º"""

    # ------------------------------------------------------------------
    # Embedding ç®¡ç†
    # ------------------------------------------------------------------

    def _get_embeddings(self):
        if self._embeddings is None:
            from skills.vector_gateway import get_shared_embeddings
            self._embeddings = get_shared_embeddings()
        return self._embeddings

    def _get_vector_dim(self) -> int:
        if self._vector_dim is None:
            vec = self._get_embeddings().embed_query(f"{self._tag}_dim_probe")
            self._vector_dim = len(vec)
        return self._vector_dim

    # ------------------------------------------------------------------
    # Collection ç®¡ç†
    # ------------------------------------------------------------------

    def _is_schema_compatible(self, collection: Collection, dim: int) -> bool:
        required = {f.name for f in self._schema_fields(dim) if f.name != "pk"}
        fields = {f.name: f for f in collection.schema.fields}
        if not required.issubset(fields.keys()):
            return False
        for name in self._vector_field_names():
            field = fields.get(name)
            if field is None or field.dtype != DataType.FLOAT_VECTOR:
                return False
            if int(field.params.get("dim", -1)) != dim:
                return False
        return True

    def _create_collection(self, dim: int) -> Collection:
        schema = CollectionSchema(
            fields=self._schema_fields(dim),
            description=self._collection_description,
            enable_dynamic_field=True,
        )
        collection = Collection(
            name=self._collection_name,
            schema=schema,
            consistency_level="Bounded",
        )

        vec_idx = {"metric_type": "COSINE",
                   "index_type": "AUTOINDEX", "params": {}}
        for field_name in self._vector_field_names():
            collection.create_index(
                field_name=field_name, index_params=vec_idx)

        # æ ‡é‡å€’æ’ç´¢å¼• (é€šç”¨å­—æ®µ)
        for f in self._schema_fields(dim):
            if f.name in ("url_pattern", "dom_hash", "cache_id") and f.dtype == DataType.VARCHAR:
                try:
                    collection.create_index(
                        field_name=f.name, index_params={"index_type": "INVERTED"})
                except Exception:
                    pass

        collection.load()
        print(
            f"âœ… [{self._tag}] Created collection '{self._collection_name}' (dim={dim})")
        return collection

    def _ensure_collection(self) -> Collection:
        if self._collection is not None:
            return self._collection
        connect_milvus(MILVUS_URI, alias="default", tag=self._tag)
        dim = self._get_vector_dim()
        name = self._collection_name

        if utility.has_collection(name):
            current = Collection(name)
            if not self._is_schema_compatible(current, dim):
                print(
                    f"âš ï¸ [{self._tag}] Incompatible schema in '{name}', dropping and recreating.")
                utility.drop_collection(name)
                current = self._create_collection(dim)
            else:
                current.load()
                print(f"ğŸ“¦ [{self._tag}] Reusing collection '{name}'")
            self._collection = current
            return self._collection

        self._collection = self._create_collection(dim)
        return self._collection

    # ------------------------------------------------------------------
    # é€šç”¨å·¥å…·æ–¹æ³•
    # ------------------------------------------------------------------

    def _normalize_url(self, url: str) -> str:
        try:
            parsed = urlparse(url)
            domain = parsed.netloc
            if domain.lower().startswith("www."):
                domain = domain[4:]
            path = re.sub(r"/\d+", "/*", parsed.path or "")
            return f"{domain}{path}"[:512]
        except Exception:
            return (url or "")[:512]

    def _compute_dom_hash(self, dom_skeleton: str, max_len: int = 2500) -> str:
        content = (dom_skeleton or "")[:max_len]
        return hashlib.md5(content.encode("utf-8")).hexdigest()[:16]

    def _to_similarity(self, score: float) -> float:
        """å°† Milvus è¿”å›çš„è·ç¦»/å¾—åˆ†ç»Ÿä¸€è½¬ä¸º [0, 1] ç›¸ä¼¼åº¦"""
        value = float(score)
        if 0.0 <= value <= 1.0:
            return value
        if 1.0 < value <= 2.0:
            return max(0.0, 1.0 - value / 2.0)
        if -1.0 <= value < 0.0:
            return max(0.0, min(1.0, 1.0 + value))
        return max(0.0, min(1.0, 1.0 / (1.0 + abs(value))))

    def record_failure(self, cache_id: str, reason: str = "") -> None:
        """è®°å½•ç¼“å­˜å‘½ä¸­å¤±è´¥ï¼ˆä¸åˆ é™¤ç¼“å­˜ï¼Œä»…åšæŒä¹…åŒ–æ ‡è®°ä¾›ç”¨æˆ·å®¡æŸ¥ï¼‰

        å¤±è´¥å¯èƒ½æ˜¯ä¸Šä¸‹æ–‡ç›¸å…³çš„ï¼ˆDOM ä¸´æ—¶å˜åŒ–ã€é¡µé¢å¼‚å¸¸ç­‰ï¼‰ï¼Œ
        ä¸ä»£è¡¨ç¼“å­˜æœ¬èº«æ˜¯åæ•°æ®ã€‚å½“å‰è½®æ¬¡çš„è·³è¿‡ç”± _cache_failed_this_round
        ç†”æ–­å™¨ä¿è¯ï¼Œæ­¤æ–¹æ³•åªè´Ÿè´£æŒä¹…åŒ–è®°å½•ã€‚

        å¤±è´¥æ—¥å¿—å†™å…¥ output/cache_failures.jsonlï¼Œç”¨æˆ·å¯å®¡æŸ¥å
        æ‰‹åŠ¨è°ƒç”¨ invalidate() åˆ é™¤ç¡®è®¤æ— æ•ˆçš„ç¼“å­˜ã€‚
        """
        if not cache_id:
            return
        import json as _json
        import os
        from datetime import datetime

        log_path = os.path.join("output", "cache_failures.jsonl")
        os.makedirs("output", exist_ok=True)

        entry = {
            "cache_id": cache_id,
            "cache_type": self._tag,
            "timestamp": datetime.now().isoformat(),
            "reason": reason,
        }
        try:
            with open(log_path, "a", encoding="utf-8") as f:
                f.write(_json.dumps(entry, ensure_ascii=False) + "\n")
            print(f"ğŸ“‹ [{self._tag}] å·²è®°å½•å¤±è´¥: {cache_id} (åŸå› : {reason})")
            print(f"   â„¹ï¸ å¦‚éœ€åˆ é™¤æ­¤ç¼“å­˜ï¼Œè¯·æ‰‹åŠ¨è°ƒç”¨ invalidate('{cache_id}')")
        except Exception as e:
            print(f"âš ï¸ [{self._tag}] è®°å½•å¤±è´¥æ—¥å¿—å¼‚å¸¸: {e}")

    def invalidate(self, cache_id: str) -> bool:
        """æ‰‹åŠ¨åˆ é™¤æŒ‡å®šç¼“å­˜ï¼ˆä»…ä¾›ç”¨æˆ·ä¸»åŠ¨æ¸…ç†æ—¶è°ƒç”¨ï¼‰"""
        if not cache_id:
            return False
        try:
            collection = self._ensure_collection()
            safe = cache_id.replace('"', '\\"')
            collection.delete(expr=f'cache_id == "{safe}"')
            print(f"ğŸ—‘ï¸ [{self._tag}] Invalidated: {cache_id}")
            return True
        except Exception as exc:
            print(f"âš ï¸ [{self._tag}] Invalidate error: {exc}")
            return False

    def _shutdown(self):
        print(f"ğŸ“§ [{self._tag}] Waiting for background tasks...")
        self._executor.shutdown(wait=True)
        print(f"âœ… [{self._tag}] Background tasks finished")
